{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7-4lpP34QFF9"
   },
   "source": [
    "# Artificial Neural Network implementation using PyTorch\n",
    "\n",
    "Importing additional libraries for neural network computing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data\n",
    "\n",
    "The task is the same as the multi-variable regression task carried out previously: estimating the Horsepower of each car based on the Engine size and Fuel efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"DataANN/Car_sales.csv\")\n",
    "df['Engine_ccm'] = df['Engine_size'] * 1000\n",
    "#'Horsepower', 'Engine_ccm', 'Fuel_efficiency', 'Price_in_thousands', 'Fuel_capacity'\n",
    "df = df.dropna(subset=['Horsepower', 'Engine_ccm', 'Fuel_efficiency'])\n",
    "X = df[['Engine_ccm','Fuel_efficiency']]\n",
    "Y = df[['Horsepower']]\n",
    "x = X.values\n",
    "y = Y.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing data\n",
    "\n",
    "In this case, the data preparation only includes the division of the entire data set into teaching validation and test parts. The entire data set must describe reality well and all three parts must describe the entire data set well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(x) * 0.6)\n",
    "valid_size = int(len(x) * 0.2)\n",
    "test_size = len(x) - train_size - valid_size\n",
    "x1=np.double(np.concatenate((np.ones(len(x)).reshape(-1,1), x), axis=1))\n",
    "x_train, x_valid, x_test = torch.tensor(x1[:train_size]), torch.tensor(x1[train_size:train_size+valid_size]),torch.tensor(x1[train_size+valid_size:])\n",
    "y_train, y_valid, y_test = torch.tensor(y[:train_size]), torch.tensor(y[train_size:train_size+valid_size]),torch.tensor(y[train_size+valid_size:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Neural network with PyTorch\n",
    "\n",
    "The Pytorch Neural Network (https://pytorch.org/docs/stable/nn.html) model class consists of two main parts:\n",
    "* Initialization - contains the layers and submodels of the model\n",
    "* Foward - contains the structure of the model\n",
    "\n",
    "Parts of a simple MLP model:\n",
    "* Linear layer - parameters: number of inputs and outputs \n",
    "* Activation layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CarANN(nn.Module):\n",
    "    def __init__(self, num_input=3, num_output=1):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(num_input, 20)\n",
    "        #self.bnorm1d = nn.BatchNorm1d(20,eps=1e-05, momentum=0.1, affine=False)\n",
    "        self.bnorm1d = nn.LazyBatchNorm1d(eps=1e-05, momentum=0.1, affine=False)\n",
    "        self.drop = nn.Dropout(p=0.2)\n",
    "        #self.act = nn.ReLU()\n",
    "        self.act = nn.Sigmoid()\n",
    "        self.linear2 = nn.Linear(20, num_output)\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        #print(x.size())\n",
    "        x = self.bnorm1d(x)\n",
    "        x = self.drop(x)\n",
    "        x=self.act(x)\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kovac\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CarANN(num_input=3, num_output=1).double().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizers and their options\n",
    "\n",
    "There are some pre-built optimizers in PyTorch (https://pytorch.org/docs/stable/optim.html), they are sufficient in most cases, especially if their parameters are well set. The two most well-known are ADAM and SGD, both of which originate from Gradient Descent, which we implemented earlier.\n",
    "\n",
    "* **S**tochastic **G**radient **D**escent  - https://pytorch.org/docs/stable/generated/torch.optim.SGD.html\n",
    "* **ADA**ptive **M**oment optimizer - https://pytorch.org/docs/stable/generated/torch.optim.Adam.html\n",
    "* A good general overview - https://www.ruder.io/optimizing-gradient-descent/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer = optim.SGD(model.parameters(), lr=0.01,momentum=0.9, dampening=0, weight_decay=0.001)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01, betas=(0.99, 0.999), eps=1e-08, weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss functions and their options\n",
    "\n",
    "Built-in cost/loss functions are also available in PyTorch (https://pytorch.org/docs/stable/nn.html#loss-functions). There are two main types: categorical and general.\n",
    "\n",
    "For a regression task, we need a general, of which I would highlight the following 3:\n",
    "* L1 norm (or absolute value) - https://pytorch.org/docs/stable/generated/torch.nn.L1Loss.html#torch.nn.L1Loss\n",
    "* Squared L2 norm (or mean squared error) - https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html#torch.nn.MSELoss\n",
    "* Smoothed L1 norm - https://pytorch.org/docs/stable/generated/torch.nn.SmoothL1Loss.html#torch.nn.SmoothL1Loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "#loss_fn = nn.L1Loss() #mean abs\n",
    "#loss_fn = nn.SmoothL1Loss(reduction='mean', beta=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloder and its options\n",
    "\n",
    "The DataLoader (https://pytorch.org/docs/stable/data.html) has 3 main parameters: \n",
    "* dataset - The dataset itself\n",
    "* batch_size - Number of evaluations used for an optimization step\n",
    "* shuffle - Regenerates batches in every epoch (cannot be used in the case of time series data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=10\n",
    "loader = data.DataLoader(data.TensorDataset(x_train, y_train), shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Training and testing a model always consists of 3 parts:\n",
    "* Training - only the training data set is used, and the weights are updated based on this\n",
    "* Validation - the validation data set is used, indirectly participates in training and serves to monitor the training process\n",
    "* Testing - the test data set is used, it does not participate in training in any way, and the goal is independent testing and comparison\n",
    "\n",
    "\n",
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Attempted to use an uninitialized parameter in <function Tensor.__deepcopy__ at 0x000001E46AE4E290>. This error happens when you are using a `LazyModule` or explicitly manipulating `torch.nn.parameter.UninitializedBuffer` objects. When using LazyModules Call `forward` with a dummy batch to initialize the parameters before calling torch functions",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m hist_valid\u001b[38;5;241m=\u001b[39m[];\n\u001b[0;32m      4\u001b[0m best_loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m best_model_wts \u001b[38;5;241m=\u001b[39m \u001b[43mcopy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m early_stop_tolerant_count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      7\u001b[0m early_stop_tolerant\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m;\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\copy.py:172\u001b[0m, in \u001b[0;36mdeepcopy\u001b[1;34m(x, memo, _nil)\u001b[0m\n\u001b[0;32m    170\u001b[0m                 y \u001b[38;5;241m=\u001b[39m x\n\u001b[0;32m    171\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 172\u001b[0m                 y \u001b[38;5;241m=\u001b[39m \u001b[43m_reconstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;66;03m# If is its own copy, don't memoize.\u001b[39;00m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m x:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\copy.py:297\u001b[0m, in \u001b[0;36m_reconstruct\u001b[1;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[0;32m    295\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m dictiter:\n\u001b[0;32m    296\u001b[0m         key \u001b[38;5;241m=\u001b[39m deepcopy(key, memo)\n\u001b[1;32m--> 297\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    298\u001b[0m         y[key] \u001b[38;5;241m=\u001b[39m value\n\u001b[0;32m    299\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\copy.py:153\u001b[0m, in \u001b[0;36mdeepcopy\u001b[1;34m(x, memo, _nil)\u001b[0m\n\u001b[0;32m    151\u001b[0m copier \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__deepcopy__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 153\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m     reductor \u001b[38;5;241m=\u001b[39m dispatch_table\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mcls\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\_tensor.py:84\u001b[0m, in \u001b[0;36mTensor.__deepcopy__\u001b[1;34m(self, memo)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__deepcopy__\u001b[39m(\u001b[38;5;28mself\u001b[39m, memo):\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m---> 84\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhandle_torch_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__deepcopy__\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_leaf:\n\u001b[0;32m     86\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m     87\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnly Tensors created explicitly by the user \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     88\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(graph leaves) support the deepcopy protocol at the moment\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     89\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\overrides.py:1551\u001b[0m, in \u001b[0;36mhandle_torch_function\u001b[1;34m(public_api, relevant_args, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1545\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDefining your `__torch_function__ as a plain method is deprecated and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1546\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwill be an error in future, please define it as a classmethod.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1547\u001b[0m                   \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m)\n\u001b[0;32m   1549\u001b[0m \u001b[38;5;66;03m# Use `public_api` instead of `implementation` so __torch_function__\u001b[39;00m\n\u001b[0;32m   1550\u001b[0m \u001b[38;5;66;03m# implementations can do equality/identity comparisons.\u001b[39;00m\n\u001b[1;32m-> 1551\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch_func_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpublic_api\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1553\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[0;32m   1554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\parameter.py:153\u001b[0m, in \u001b[0;36mUninitializedTensorMixin.__torch_function__\u001b[1;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m__torch_function__(func, types, args, kwargs)\n\u001b[1;32m--> 153\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAttempted to use an uninitialized parameter in \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThis error happens when you are using a `LazyModule` or \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexplicitly manipulating `torch.nn.parameter.\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m` \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobjects. When using LazyModules Call `forward` with a dummy batch \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mto initialize the parameters before calling torch functions\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(func, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m))\n",
      "\u001b[1;31mValueError\u001b[0m: Attempted to use an uninitialized parameter in <function Tensor.__deepcopy__ at 0x000001E46AE4E290>. This error happens when you are using a `LazyModule` or explicitly manipulating `torch.nn.parameter.UninitializedBuffer` objects. When using LazyModules Call `forward` with a dummy batch to initialize the parameters before calling torch functions"
     ]
    }
   ],
   "source": [
    "n_epochs = 1000\n",
    "hist_train=[];\n",
    "hist_valid=[];\n",
    "best_loss=float('inf')\n",
    "best_model_wts = copy.deepcopy(model.state_dict())\n",
    "early_stop_tolerant_count=0\n",
    "early_stop_tolerant=10;\n",
    "for epoch in range(n_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    for X_batch, y_batch in loader:\n",
    "        \n",
    "        y_pred = model(X_batch.to(device))\n",
    "        train_loss = loss_fn(y_pred, y_batch.to(device))\n",
    "        train_loss=train_loss/batch_size\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        valid_loss=0\n",
    "        for i in range(0,valid_size-1):\n",
    "            y_pred = model(x_valid[i].unsqueeze(0).to(device)) # in case of batch normalisation\n",
    "            #y_pred = model(x_valid[i].to(device))\n",
    "            valid_loss = valid_loss+torch.sqrt(loss_fn(y_pred, y_valid[i].to(device)))\n",
    "        valid_loss=valid_loss/valid_size                        \n",
    "        hist_train.append(np.array(train_loss.cpu().detach()))\n",
    "        hist_valid.append(np.array(valid_loss.cpu().detach()))\n",
    "        \n",
    "    # Always save the current best model based on the validation data, and stop the training if no improvements happen after a certain epoch.    \n",
    "    early_stop_tolerant_count=early_stop_tolerant_count+1\n",
    "    if valid_loss < best_loss:\n",
    "        early_stop_tolerant_count=0\n",
    "        best_loss = valid_loss\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    if early_stop_tolerant_count>=early_stop_tolerant:\n",
    "        break \n",
    "    if epoch % 50 != 0:\n",
    "        continue       \n",
    "    # Display some information along the training\n",
    "    print(\"Epoch %.4d: train loss %.2f, valid loss %.2f\" % (epoch, train_loss, valid_loss))\n",
    "    \n",
    "    \n",
    "#Test    \n",
    "model.load_state_dict(best_model_wts)\n",
    "test_loss=0\n",
    "model.eval()\n",
    "for i in range(0,test_size-1):\n",
    "    y_pred = model(x_test[i].unsqueeze(0).to(device)) # in case of batch normalisation\n",
    "    #y_pred = model(x_test[i].to(device))\n",
    "    test_loss = test_loss+torch.sqrt(loss_fn(y_pred, y_test[i].to(device)))\n",
    "test_loss=test_loss/test_size\n",
    "\n",
    "print(\"Final epoch %d: train loss %.2f, valid loss %.2f, test loss %.2f\" % (epoch, train_loss, valid_loss, test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "plt.figure(figsize=(10,6))\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss by Iteration')\n",
    "plt.plot(hist_train)\n",
    "plt.plot(hist_valid)\n",
    "plt.plot(test_loss.cpu().detach().numpy()*np.ones(len(hist_valid)))\n",
    "plt.ylim((0,10000))\n",
    "plt.legend(['train', 'valid', 'test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Homework: Try different cost functions and other parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\">Created by Szilárd Kovács</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Lab2 - Deep Network Developments.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
